{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "import wordcloud\n",
    "\n",
    "import spacy\n",
    "#spacy.cli.download(\"it_core_news_sm\")\n",
    "ITA_tokenizer = spacy.load(\"it_core_news_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "num_topics = 4\n",
    "\n",
    "it_stop_words = []#stopwords.words('italian')\n",
    "custom_stop_words = ['me','xe', 'el', 'lettera', 'schmitz','ettore', 'signora', 'signore', 'scrivere', 'sapere','fare', 'cosa','essere']\n",
    "it_stop_words.extend(custom_stop_words)\n",
    "full_corpus = pd.read_csv('carteggio_svevo3.csv', sep=';')\n",
    "ita_corpus = full_corpus[full_corpus['mainLanguage'] == 'ITA'].reset_index()\n",
    "corpus_for_tfidf = full_corpus[full_corpus['mainLanguage'] == 'ITA'].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the text of the project proposal the goal is to find:\n",
    "- which are the main topics of discussion in the corpus, who are the people which each topic is more associated with, how does the interest on different topics evolve over the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc(text, pos = ['PROPN', 'NOUN', 'VERB'], stop_words = it_stop_words ):\n",
    "    #for the tokenization part we used a piece of software from spacy that performs tokenization, lemmatization\n",
    "    #and pos tagging in one shot\n",
    "    \n",
    "    text = text.lower()\n",
    "    doc = ITA_tokenizer(text)\n",
    "    tokenized_text = []\n",
    "    #tokenizing and lemmatizing on one shot\n",
    "    for token in doc:\n",
    "        if token.lemma_ not in stop_words and token.pos_ in pos and not token.is_stop and token.is_alpha:\n",
    "            tokenized_text.append(token.lemma_)\n",
    "            #print(token.pos_)\n",
    "    return tokenized_text\n",
    "\n",
    "def extract_topic_keywords(model, num_words, plot = True):\n",
    "    #extracting for each topic the keywords and making some plots\n",
    "    num_topics = len(model.print_topics())\n",
    "    if plot:\n",
    "        for t,w  in lda_model_tfidf.show_topics(formatted=False, num_topics=num_topics, num_words=num_words):\n",
    "            \n",
    "            print(\"Topic {} keywords:\\n\\n format['word','frequency']\\n\\n {}\".format(t,w))\n",
    "\n",
    "            w = dict(w)\n",
    "            width = 800\n",
    "            height = 600\n",
    "\n",
    "            wcloud = wordcloud.WordCloud(width = width, height = height, background_color='White').generate_from_frequencies(w)\n",
    "            plt.figure(dpi=150)\n",
    "            plt.imshow(wcloud)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    \n",
    "    \n",
    "    return [[tup2[0] for tup2 in tup[1]]for tup in model.show_topics(formatted=False, num_topics=num_topics, num_words=num_words)]\n",
    "\n",
    "def get_topic_scores(model, bow_corpus):\n",
    "    #getting for each topic the score assigned for each topic\n",
    "    corpus_scores = []\n",
    "    for letter in bow_corpus:\n",
    "        score = [0 for i in range(num_topics)]\n",
    "        model_output =  model[letter]\n",
    "        \n",
    "        for out in model_output:\n",
    "            i = out[0]\n",
    "            score[i] = out[1]\n",
    "        \n",
    "        corpus_scores.append(score)\n",
    "    return corpus_scores\n",
    "\n",
    "def get_coherence(model, corpus, dictionary):\n",
    "    \n",
    "    c_av = models.CoherenceModel(model=model, corpus = bow,  dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    c_per_topic = models.CoherenceModel(model=model, corpus = bow,  dictionary=dictionary, coherence='u_mass').get_coherence_per_topic()\n",
    "    return c_av,c_per_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Livia Veneziani               608\n",
       "OTHER                          63\n",
       "Eugenio Montale                62\n",
       "Paul Henri Michel              17\n",
       "Valerio Jahier                 15\n",
       "Marieanne Crémieux Comnène     14\n",
       "James Joyce                    13\n",
       "Giuseppe Prezzolini            12\n",
       "Ferdinando Pasini              11\n",
       "Valéry Larbaud                 11\n",
       "Name: SR, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sender = ita_corpus['sender']\n",
    "recipient = ita_corpus['recipient']\n",
    "important_people = ['Livia Veneziani','Eugenio Montale','Paul Henri Michel','Valerio Jahier','Marieanne Crémieux Comnène','James Joyce','Giuseppe Prezzolini','Ferdinando Pasini','Valéry Larbaud']\n",
    "sr = []\n",
    "for i in range(len(sender)):\n",
    "    if sender[i] != 'Ettore Schmitz' and sender[i] in important_people:\n",
    "        sr.append(sender[i])\n",
    "    elif recipient[i] in important_people:\n",
    "        sr.append(recipient[i])\n",
    "    else:\n",
    "        sr.append('OTHER')\n",
    "ita_corpus['SR'] = sr  \n",
    "ita_corpus['SR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Livia Veneziani               608\n",
       "OTHER                          63\n",
       "Eugenio Montale                62\n",
       "Paul Henri Michel              17\n",
       "Valerio Jahier                 15\n",
       "Marieanne Crémieux Comnène     14\n",
       "James Joyce                    13\n",
       "Giuseppe Prezzolini            12\n",
       "Ferdinando Pasini              11\n",
       "Valéry Larbaud                 11\n",
       "Name: SR, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sender = corpus_for_tfidf['sender']\n",
    "recipient = corpus_for_tfidf['recipient']\n",
    "important_people = ['Livia Veneziani','Eugenio Montale','Paul Henri Michel','Valerio Jahier','Marieanne Crémieux Comnène','James Joyce','Giuseppe Prezzolini','Ferdinando Pasini','Valéry Larbaud']\n",
    "sr = []\n",
    "for i in range(len(sender)):\n",
    "    if sender[i] != 'Ettore Schmitz' and sender[i] in important_people:\n",
    "        sr.append(sender[i])\n",
    "    elif recipient[i] in important_people:\n",
    "        sr.append(recipient[i])\n",
    "    else:\n",
    "        sr.append('OTHER')\n",
    "corpus_for_tfidf['SR'] = sr  \n",
    "corpus_for_tfidf['SR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_letters = []\n",
    "for letter in ita_corpus['text']:\n",
    "    \n",
    "\n",
    "    tokenized_letter = pre_proc(letter,pos = ['PROPN', 'NOUN', 'VERB'])\n",
    "    processed_letters.append(tokenized_letter)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "pure_dictionary = gensim.corpora.Dictionary(processed_letters)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_letters)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "dictionary_for_tfidf = gensim.corpora.Dictionary(processed_letters)\n",
    "\n",
    "pruned_dictionary_for_tfidf = gensim.corpora.Dictionary(processed_letters)\n",
    "pruned_dictionary_for_tfidf.filter_extremes(no_below=5, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_bow = [pure_dictionary.doc2bow(doc) for doc in processed_letters] \n",
    "\n",
    "bow = [dictionary.doc2bow(doc) for doc in processed_letters]\n",
    "\n",
    "bow_for_tfidf=[dictionary_for_tfidf.doc2bow(doc) for doc in processed_letters]\n",
    "\n",
    "bow_for_pruned_tfidf=[pruned_dictionary_for_tfidf.doc2bow(doc) for doc in processed_letters]\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_for_tfidf)\n",
    "corpus_for_tfidf = tfidf[bow_for_tfidf]\n",
    "\n",
    "pruned_tfidf = models.TfidfModel(bow_for_pruned_tfidf)\n",
    "corpus_for_pruned_tfidf = tfidf[bow_for_pruned_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating coherence scores for 1 topics\n",
      "[-9.249616584548876, -1.1106195357148791, -8.633821591997556, -1.2399792263626386]\n",
      "Calculating coherence scores for 2 topics\n"
     ]
    }
   ],
   "source": [
    "max_n_topics = 4\n",
    "\n",
    "coherency_scores = np.zeros([max_n_topics,4])\n",
    "\n",
    "for n in range(1,max_n_topics + 1):\n",
    "    print(\"Calculating coherence scores for {} topics\".format(n))\n",
    "    #learn a model without pruning the dictionary\n",
    "    lda_model_unpruned = gensim.models.LdaMulticore(pure_bow, num_topics=n, id2word=pure_dictionary, passes=10)\n",
    "    #learn a model pruning the dictionary\n",
    "    lda_model_pruned = gensim.models.LdaMulticore(bow, num_topics=n, id2word=dictionary, passes=10)\n",
    "    #learn a model with tf_idf\n",
    "    lda_model_tfidf = gensim.models.LdaMulticore(corpus_for_tfidf, num_topics=n, id2word=dictionary_for_tfidf, passes=10)\n",
    "    #learn a model with tf_idf on a pruned dictionary\n",
    "    lda_model_pruned_tfidf = gensim.models.LdaMulticore(corpus_for_pruned_tfidf, num_topics=n, id2word=pruned_dictionary_for_tfidf, passes=10)\n",
    "\n",
    "    c1, _ = get_coherence(lda_model_unpruned, pure_bow, pure_dictionary)\n",
    "    c2, _ = get_coherence(lda_model_pruned, bow, pure_dictionary)\n",
    "    c3, _ = get_coherence(lda_model_tfidf, corpus_for_tfidf, dictionary_for_tfidf)\n",
    "    c4, _ = get_coherence(lda_model_pruned_tfidf, corpus_for_pruned_tfidf, pruned_dictionary_for_tfidf)\n",
    "    print([c1,c2,c3,c4])\n",
    "    coherency_scores[n - 1,:] = np.array([c1,c2,c3,c4])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['classical bow no pruning', 'classical bow pruned dictionary', 'tfidf', 'tfidf pruned']\n",
    "n_topics = [i for i in range(1,max_n_topics+1)]\n",
    "np.shape(coherency_scores)\n",
    "plt.figure(dpi=120)\n",
    "for i in range(4):\n",
    "    plt.plot(n_topics, coherency_scores[:,i], '.-', label = model_names[i])\n",
    "    \n",
    "_ = plt.legend()\n",
    "_ = plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow, num_topics=num_topics, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=num_topics, id2word=dictionary_for_tfidf, passes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key_words = extract_topic_keywords(lda_model, 20)\n",
    "\n",
    "for i in range(len(key_words)):\n",
    "    print(\"Topic {}:\\n\\n{}\\n\".format(i,key_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_topic_scores(lda_model, bow)\n",
    "letter_topics = list(map(np.argmax, scores))\n",
    "ita_corpus['topic_scores'] = scores\n",
    "ita_corpus['topic'] = letter_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "people = []\n",
    "important_people = ['Livia Veneziani','Eugenio Montale','Paul Henri Michel','Valerio Jahier','Marieanne Crémieux Comnène','James Joyce','Giuseppe Prezzolini','Ferdinando Pasini','Valéry Larbaud']\n",
    "\n",
    "people = important_people\n",
    "people.append('OTHER')\n",
    "\n",
    "for person in people:\n",
    "    sub_data_index = ita_corpus['SR'] == person\n",
    "    sub_data = ita_corpus[sub_data_index]\n",
    "    val_count = sub_data['topic'].value_counts()\n",
    "    print(\"Person: {}\".format(person))\n",
    "    print(\"Topics\")\n",
    "    print(val_count)\n",
    "    print(\"--- --- --- --- \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = ita_corpus['year']\n",
    "min_y = min(year)\n",
    "max_y = max(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_per_year = []\n",
    "for y in range(min_y,max_y+1):\n",
    "    counts = [0 for i in range(num_topics)]\n",
    "    tpy = ita_corpus[ita_corpus['year'] == y]['topic']\n",
    "    for topic in tpy:\n",
    "        counts[topic] += 1 \n",
    "    topic_per_year.append(counts)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = [y for y in range(min_y,max_y+1)]\n",
    "topic_per_year = np.array(topic_per_year, dtype = float)\n",
    "plt.figure(dpi = 100)\n",
    "for topic in range(num_topics):\n",
    "    plt.plot(yy, topic_per_year[:,topic], '.-', label = \"Topic\" + str(topic))\n",
    "plt.legend()\n",
    "plt.figure(dpi = 100)\n",
    "\n",
    "topic_per_year_norm = np.zeros_like(topic_per_year)\n",
    "\n",
    "for i in range(len(topic_per_year)):\n",
    "    if np.sum(topic_per_year[i,:]) != 0:\n",
    "       \n",
    "        topic_per_year_norm[i,:] = topic_per_year[i,:]/np.sum(topic_per_year[i,:])\n",
    "        #print(np.sum(topic_per_year[i,:]), topic_per_year_norm[i,:])\n",
    "\n",
    "\n",
    "\n",
    "for topic in range(num_topics):\n",
    "    plt.plot(yy, topic_per_year_norm[:,topic], '.-', label = \"Topic\" + str(topic))\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the identified topics understandable?\n",
    "\n",
    "Are the topics coherent?\n",
    "\n",
    "Does the topic model serve the purpose it is being used for?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key_words = extract_topic_keywords(lda_model_tfidf, 20)\n",
    "\n",
    "for i in range(len(key_words)):\n",
    "    print(\"Topic {}:\\n\\n{}\\n\".format(i,key_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_topic_scores(lda_model_tfidf, bow_for_tfidf)\n",
    "letter_topics = list(map(np.argmax, scores))\n",
    "corpus_for_tfidf['topic_scores'] = scores\n",
    "corpus_for_tfidf['topic'] = letter_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_per_year = []\n",
    "for y in range(min_y,max_y+1):\n",
    "    counts = [0 for i in range(num_topics)]\n",
    "    tpy = corpus_for_tfidf[corpus_for_tfidf['year'] == y]['topic']\n",
    "    for topic in tpy:\n",
    "        counts[topic] += 1 \n",
    "    topic_per_year.append(counts)\n",
    "\n",
    "\n",
    "yy = [y for y in range(min_y,max_y+1)]\n",
    "topic_per_year = np.array(topic_per_year, dtype = float)\n",
    "plt.figure(dpi = 100)\n",
    "for topic in range(num_topics):\n",
    "    plt.plot(yy, topic_per_year[:,topic], '.-', label = \"Topic\" + str(topic))\n",
    "plt.legend()\n",
    "plt.figure(dpi = 100)\n",
    "\n",
    "topic_per_year_norm = np.zeros_like(topic_per_year)\n",
    "\n",
    "for i in range(len(topic_per_year)):\n",
    "    if np.sum(topic_per_year[i,:]) != 0:\n",
    "       \n",
    "        topic_per_year_norm[i,:] = topic_per_year[i,:]/np.sum(topic_per_year[i,:])\n",
    "        #print(np.sum(topic_per_year[i,:]), topic_per_year_norm[i,:])\n",
    "\n",
    "\n",
    "\n",
    "for topic in range(num_topics):\n",
    "    plt.plot(yy, topic_per_year_norm[:,topic], '.-', label = \"Topic\" + str(topic))\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = []\n",
    "important_people = ['Livia Veneziani','Eugenio Montale','Paul Henri Michel','Valerio Jahier','Marieanne Crémieux Comnène','James Joyce','Giuseppe Prezzolini','Ferdinando Pasini','Valéry Larbaud']\n",
    "\n",
    "people = important_people\n",
    "people.append('OTHER')\n",
    "\n",
    "for person in people:\n",
    "    sub_data_index = corpus_for_tfidf['SR'] == person\n",
    "    sub_data = corpus_for_tfidf[sub_data_index]\n",
    "    val_count = sub_data['topic'].value_counts()\n",
    "    print(\"Person: {}\".format(person))\n",
    "    print(\"Topics\")\n",
    "    print(val_count)\n",
    "    print(\"--- --- --- --- \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.CoherenceModel(model=lda_model, corpus = bow,  dictionary=dictionary, coherence='u_mass').get_coherence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.CoherenceModel(model=lda_model_tfidf, corpus = bow_for_tfidf,  dictionary=dictionary_for_tfidf, coherence='u_mass').get_coherence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.CoherenceModel(model=lda_model, corpus = bow,  dictionary=dictionary, coherence='u_mass').get_coherence_per_topic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.CoherenceModel(model=lda_model_tfidf, corpus = bow_for_tfidf,  dictionary=dictionary_for_tfidf, coherence='u_mass').get_coherence_per_topic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
